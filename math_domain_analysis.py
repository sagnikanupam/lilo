from __future__ import annotations
import pandas as pd
import json
import dreamcoder.domains.math.mathPrimitives as mdp

ORIGINAL_DATASET_FILEPATHS = ['meta_analysis/MathDomainAnalysis/conpoleDatasetPrefix.csv']
GOLDEN_DATASET_FILEPATHS = ['meta_analysis/MathDomainAnalysis/goldenDataset.csv']
GENERATED_CONPOLE_SOLNS = 'meta_analysis/MathDomainAnalysis/generatedConpoleSolutions-CScores.csv' #After compute metrics has been called on the solutions
GENERATED_LEMMA_SOLNS = 'meta_analysis/MathDomainAnalysis/generatedLemmaSolutions-CScores.csv' #After compute metrics has been called on the solutions
REGULAR_DREAMCODER_STITCH_MATHDSL_JSON = 'meta_analysis/MathDomainAnalysis/regular_dreamcoder_stitch_mathdsl.json'
WEIGHTED_DREAMCODER_STITCH_MATHDSL_JSON = 'meta_analysis/MathDomainAnalysis/weighted_dreamcoder_stitch_mathdsl.json'
GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON = 'meta_analysis/MathDomainAnalysis/golden_dreamcoder_stitch_mathdsl.json'
AUGMENTED_WITH_GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON = 'meta_analysis/MathDomainAnalysis/augmented_with_golden_dreamcoder_stitch_mathdsl.json'
AUTO_AUGMENTATION_JSON = []
ORIGINAL_TRAIN_SPLIT = 0.7
RANDOM_SEED = 111

functions_dict = {
    '(_swap': (mdp._swap, 2), 
    '(_add': (mdp._add, 2), 
    '(_sub': (mdp._sub, 2), 
    '(_mult': (mdp._mult, 2), 
    '(_div': (mdp._div, 2), 
    '(_rrotate': (mdp._rrotate, 2), 
    '(_lrotate': (mdp._lrotate, 2), 
    '(_simplify': (mdp._simplify, 2), 
    '(_dist': (mdp._dist, 2),
    '(_revdist': (mdp._revdist, 2),
    '(_addzero': (mdp._addzero, 2),
    '(_subzero': (mdp._subzero, 2),
    '(_multone': (mdp._multone, 2),
    '(_divone': (mdp._divone, 2),
    '(_newConstGen': (mdp._newConstGen, 3),
}

def replacements(s: str):
    """
    Given a string of a program generated by DreamCoder, replaces all mathDomain primitive names with their corresponding functions_dict index name.

    Arguments: 
        s(str): contains a lambda expression describing a program generated by DreamCoder which is a series of functions composed of primitives. E.g. - (lambda (#(lambda (mathDomain_simplify (mathDomain_dist $0 1) 0)) $0)) 
    
    Returns:
        a string with all the mathDomain primitive names replaced. This output shall henceforth be referred to as a solution program expression.

    """
    replacements_dict = {'mathDomain_swap': '_swap', 'mathDomain_add': '_add', 'mathDomain_sub': '_sub', 'mathDomain_mult': '_mult', 'mathDomain_div': '_div', 'mathDomain_rrotate': '_rrotate', 'mathDomain_lrotate': '_lrotate', 'mathDomain_simplify': '_simplify', 'mathDomain_dist':'_dist', 'mathDomain_revdist': '_revdist', 'mathDomain_addzero': '_addzero', 'mathDomain_subzero': '_subzero', 'mathDomain_multone': '_multone', 'mathDomain_divone': '_divone', 'mathDomain_newConstGen': '_newConstGen'}
    for i in range(10):
        replacements_dict['mathDomain_'+str(i)] = str(i)
    for or_text in replacements_dict.keys():
        s = s.replace(or_text, replacements_dict[or_text])
    return s

def clSolnEval(clSolnPath, outputSolnPath):
    '''
    Given a .csv file containing solutions generated by ConPoLe or Lemma, convert the solutions into lists of steps in prefix notation, and then compute each solution's metric function.
    
    Arguments:
        clSolnPath (str): A string containing a path to the .csv file containing the ConPoLe/Lemma solutions.
        outputSolnPath (str): A string containing a path to the .csv where we would like to store our ConPoLe/Lemma solutions alongside their metric function values.

    Returns:
        None
    '''
    df = pd.read_csv(clSolnPath)
    metrics = []
    for i in range(df.shape[0]):
        str_soln = df.loc[i]['soln']
        str_soln = str_soln.replace("[", "(")
        str_soln = str_soln.replace("]", ")")
        str_eq = df.loc[i]['eqn']
        steps = str_soln.split('|')
        steps = [str_eq]+steps
        prefix_steps = [mdpc.infix_to_prefix_conversion(step) for step in steps]
        metrics.append(computeMetrics(prefix_steps))
    df['metrics'] = metrics
    df.to_csv(outputSolnPath)

def matchBr(s: str, ind: int) -> int | None:
    """
    Given an opening bracket at position ind in string s, find the  position of the corresponding closing bracket.

    Arguments: 
    s (str): denoting the solution program expression (already processed by replacements())
    ind (int): is an integer denoting the starting position of the start bracket '('

    Returns: 
    int | None: an integer denoting position of closing bracket. If start index does not have an open bracket or no closing brackets close the starting bracket, returns None.
    """
    brPair = 0
    for j in range(ind, len(s)):
        if s[j]=="(":
            brPair+=1
        if s[j]==")":
            brPair-=1
        if brPair==0:
            if j==ind:
                return None
            return j
    return None

def evaluate(s: str, args: list[str], depth: int):
    """
    Given a solution program expression, generate mathematical solutions as a list of prefix expressions, stored as a string separated by |.

    Arguments: 
        s (str): A string denoting the solution program expression (already processed by replacements())
        args (list[str]): A string containing an equation as a prefix-tree expression. This is the equation to be solved.
        depth (int): A parameter to modulate granularity of solutions, only functions with #lambdas<depth have the results of their argument included in the solution
    
    Returns:
        tuple: a tuple (list of evaluated function arguments, prefix expression generated by evaluating the function).
    """
    init_split = s.split(" ")
    if s=="":
        return args
    if args == [] or args[0]=="":
        return ([s], [s])
    if init_split[0]=="$1":
        return ([str(args[1])], [str(args[1])])
    if init_split[0] == "$0":
        return ([str(args[0])], [str(args[0])])
    if init_split[0].isnumeric():
        return ([str(int(init_split[0]))], [str(int(init_split[0]))])
    if init_split[0]=="(lambda" or init_split[0]=="(#(lambda":
        subExpStart = 8 if init_split[0]=="(lambda" else 2
        funcEnd = matchBr(s, subExpStart)
        if funcEnd!=None:
            funcArgs = []
            newFunc = s[subExpStart:funcEnd+1]
            currArgStart = funcEnd+2
            while (currArgStart < len(s)-1):
                if s[currArgStart]=="(":
                    argEnd = matchBr(s, currArgStart)
                else:
                    argEnd = None
                if argEnd==None:
                    if currArgStart!=len(s)-1:
                        newArg =  s[currArgStart:-1].split(" ")[0]
                        funcArgs.append(newArg)
                        currArgStart += len(newArg)+1 
                        continue
                    else:
                        break
                else:
                    funcArgs.append(s[currArgStart:argEnd+1])
                    currArgStart = argEnd+2
            #print(f"Arguments to the function {s[subExpStart:funcEnd+1]} are: {funcArgs} \n")
            currFuncArgs = []
            for funcArg in funcArgs:
                currFuncArgs += evaluate(funcArg, args, depth+1)[0]
            #print(f"Evaluated arguments to the function {s[subExpStart:funcEnd+1]} are: {currFuncArgs} \n")
            if currFuncArgs == []:
                return evaluate(newFunc, args, depth)
            else:
                result_tuple = evaluate(newFunc, currFuncArgs, depth+1)
                result_tuple[1].append(currFuncArgs[0])
                return result_tuple
        else:
            newFunc = s[subExpStart:-1]
            #print(f"New Func is: {newFunc} \n")
            return evaluate(newFunc, args, depth+1)
    else:
        if init_split[0] in functions_dict.keys():
            currArgStart = len(init_split[0])+1
            funcArgs = []
            while (currArgStart < len(s)-1):
                if s[currArgStart]=="(":
                    argEnd = matchBr(s, currArgStart)
                else:
                    argEnd = None
                if argEnd==None:
                    if currArgStart!=len(s)-1:
                        newArg =  s[currArgStart:-1].split(" ")[0]
                        funcArgs.append(newArg)
                        currArgStart += len(newArg)+1 
                        continue
                    else:
                        break
                else:
                    funcArgs.append(s[currArgStart:argEnd+1])
                    currArgStart = argEnd+2
            #print(f"Arguments to the function {init_split[0]} are: {funcArgs} \n")
            currFuncArgs = []
            for funcArg in funcArgs:
                currFuncArgs += evaluate(funcArg, args, depth+1)[0]
            if funcArgs == ['$0', '$1']:
                currFuncArgs = [currFuncArgs[1], currFuncArgs[0]]
            #print(f"Evaluated arguments to the function {init_split[0]} are: {currFuncArgs} \n")
            if len(currFuncArgs)==functions_dict[init_split[0]][1]:
                if functions_dict[init_split[0]][1]==2:
                    try:
                        return ([functions_dict[init_split[0]][0](currFuncArgs[0], int(currFuncArgs[1]))], [functions_dict[init_split[0]][0](currFuncArgs[0], int(currFuncArgs[1]))])
                    except Exception as e:
                        print(f"Error encountered while evaluating {init_split[0]} with args {[currFuncArgs[0], int(currFuncArgs[1])]}. \n")
                        print(e)
                        return ([args[0]], [args[0]])
                elif functions_dict[init_split[0]][1]==3:
                    try:
                        return ([functions_dict[init_split[0]][0](int(currFuncArgs[0]), int(currFuncArgs[1]), int(currFuncArgs[2]))], [functions_dict[init_split[0]][0](int(currFuncArgs[0]), int(currFuncArgs[1]), int(currFuncArgs[2]))])
                    except Exception as e:
                        print(f"Error encountered while evaluating {init_split[0]} with args {[currFuncArgs[0], int(currFuncArgs[1]), int(currFuncArgs[2])]}. \n")
                        print(e)
                        return ([args[0]], [args[0]])
            else:
                #print(f"Miscounted arguments for {init_split[0]} with args {args} \n")
                raise Exception("Miscounted arguments")
        else:
            return ([s], [s])

def evaluate_old(s: str, args: list[str], depth: int):
    """
    Given a solution program expression, generate mathematical solutions as a list of prefix expressions, stored as a string separated by |.

    Arguments: 
        s (str): A string denoting the solution program expression (already processed by replacements())
        args (list[str]): A string containing an equation as a prefix-tree expression. This is the equation to be solved.
        depth (int): A parameter to modulate granularity of solutions, only functions with #lambdas<depth have the results of their argument included in the solution

    Returns:
        list: a list comprising of all the steps of the DreamCoder solution i.e. all the steps we generate after evaluating abstractions 
    """
    init_split = s.split(" ")
    if args==[]:
        return [s]
    arg = args[0]
    if s=="":
        return [arg]
    if arg=="":
        return [s]
    if init_split[0][0]=="$":
        #return [str(arg)]
        return [str(args[int(init_split[0][1:])])]
    if init_split[0].isnumeric():
        return [str(int(init_split[0]))]
    if init_split[0]=="(lambda" or init_split[0]=="(#(lambda":
        subExpStart = 8 if init_split[0]=="(lambda" else 2
        subExpEnd = -1
        funcEnd = matchBr(s, subExpStart)
        if funcEnd!=None: 
            argEnd =  matchBr(s, funcEnd+2)
            if argEnd!=None:
                newFunc = s[subExpStart:funcEnd+1]
                newArgFunc = s[funcEnd+2:argEnd+1]
                print(f"New Func is: {newFunc}")
                print(f"New ArgFunc is: {newArgFunc}")
                print(f"Currently, func is: {s}")
                print(f"Currently, args is: {args}\n")
                evalArg = evaluate(newArgFunc, args, depth+1)
                print(f"Evaluated argument of {s} and {args} is {evalArg}")
                if depth<5:
                    return [evalArg]+evaluate(newFunc, args + [evalArg[-1]], depth+1) #evalArg is appended as it is an output produced by an abstraction
                else:
                    return evaluate(newFunc, args + [evalArg[-1]], depth+1) 
            else:
                newFunc = s[subExpStart:funcEnd+1]
                newArgFunc = s[funcEnd+2:subExpEnd]
                print(f"New Func is: {newFunc}")
                print(f"New ArgFunc is: {newArgFunc}")
                print(f"Currently, func is: {s}")
                print(f"Currently, arga is: {args}\n")
                evalArg = evaluate(newArgFunc, args, depth+1)
                print(f"Evaluated argument of {s} and {args} is {evalArg}")
                return evaluate(newFunc, args + [evalArg[-1]], depth+1) #evalArg is not appended as it is simply a substring
        else:
            newFunc = s[subExpStart:-1]
            print("New Func is: "+newFunc)
            return evaluate(newFunc, args, depth+1)
    
    if init_split[0] in functions_dict.keys():
        if init_split[1][0]=="(":
            arg1Start = len(init_split[0])+1
            arg1End = matchBr(s, arg1Start)
            arg2Start = arg1End + 2
            arg2End = -1
            if s[arg2Start]=="(":
                arg2End = matchBr(s, arg2Start)
            newArg1 = s[arg1Start:arg1End+1]
            newArg2 = s[arg1End+2:arg2End]
            print(f"New Arg1 is: {newArg1}")
            print(f"New Arg2 is: {newArg2}")
            print(f"Currently, func is {s}")
            print(f"Currently, args is: {args} \n")
            evalNewArg2 = evaluate(newArg2, args, depth+1)
            evalNewArg1 = evaluate(newArg1, args + evalNewArg2, depth+1)
            print(f"Evaluation: {s} and {args} result in " + str(evalNewArg1[-1]) + " and " + str(evalNewArg2[-1]))
            #return evalNewArg1 + [functions_dict[init_split[0]](evalNewArg1[-1], evalNewArg2[-1])] #evalNewArg1 is appended as it is an output produced by an abstraction
            if evalNewArg2[-1].isnumeric():
                evalNewArgForFunc = int(evalNewArg2[-1])
            else:
                evalNewArgForFunc = evalNewArg2[-1]
            return [functions_dict[init_split[0]](evalNewArg1[-1], evalNewArgForFunc)]
        else:
            newArg1 = init_split[1]
            newArg2 = init_split[2][:-1]
            if (init_split[2][0]=="("):
                arg2Start = len(init_split[0])+len(init_split[1])+2
                newArg2 = s[arg2Start:matchBr(s, arg2Start)+1]
            print(f"New Arg1 is: {newArg1}")
            print(f"New Arg2 is: {newArg2}")
            print(f"Currently, func is {s}")
            print(f"Currently, arg is: {args} \n")
            evalNewArg2 = evaluate(newArg2, args, depth+1)
            evalNewArg1 = evaluate(newArg1, args + evalNewArg2, depth+1)
            print(f"Evaluation: {s} and {args} result in " + str(evalNewArg1[-1]) + " and " + str(evalNewArg2[-1]))
            if evalNewArg2[-1].isnumeric():
                evalNewArgForFunc = int(evalNewArg2[-1])
            else:
                evalNewArgForFunc = evalNewArg2[-1]
            return [functions_dict[init_split[0]](evalNewArg1[-1], evalNewArgForFunc)] #evalNewArg1 is not appended as it is simply a substring
    return [s]

def computeMetrics(steps):
    """
    Takes a list of solution steps (in prefix format) as input and computes their conciseness metric function value, f(s)

    Arguments:
        steps (list[str]): A list of strings containing equations in prefix format, which can be accepted by mdp.treefy(). The steps must be in order i.e. steps[0] must be the first step in the solution, steps[1] must be the second step in the solution and so on.

    Returns:
        int: an integer denoting the total value of the metric function for that equation
    """
    total_metric = 0
    for ind in range(1, len(steps)):
        total_metric+=mdp._metric(steps[ind-1],steps[ind])
    return total_metric

def computeDreamCoderCScores(test_problem_df: pd.DataFrame, model_result_json: str, baseline_model_result_df: pd.DataFrame, experiment_name: str):
    """
    Compute the average conciseness scores of a DreamCoder experiment on the problems solved by the baseline model (ConPoLe/Lemma) in baseline_model_result_df. Ignore any problems solved by DreamCoder that are not solved by the baseline model. 

    Args:
        test_problem_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        model_result_json (str): JSON filename (frontiers.json in the last iteration of the experiment) containing results of DreamCoder.
        baseline_model_result_df (pd.DataFrame): Dataframe containing results of Conpole or Lemma.
        experiment_name (str): experiment name to be printed.

    Returns:
        float: Average conciseness score of the DreamCoder experiment on the problems solved by the baseline model.
        
    """
    total_metric = 0
    mutual_problems = 0
    test_problem_df["tmp"] = test_problem_df.index
    test_problem_df["dreamcoder_steps"] = ["unknown"]*len(test_problem_df)
    test_problem_df["dreamcoder_metrics"] = ["unknown"]*len(test_problem_df)
    test_problem_df["dreamcoder_cscores"] = ["unknown"]*len(test_problem_df)
    model_result_equation_nums = baseline_model_result_df["Equation Number"].to_list()
    test_problem_df.reset_index(drop=True, inplace=True)
    dreamcoder_solved_problems = []
    with open(model_result_json, "r") as f:
        model_result = json.load(f)
        for problem in model_result["train"].keys():
            equation = model_result["train"][problem]["task"].split("=>")[0]
            if len(model_result["train"][problem]["programs"]) > 0:
                dreamcoder_solved_problems.append((equation[len(equation.split(" ")[0])+1:], model_result["train"][problem]["programs"][0]["program"]))
        for problem in model_result["test"].keys():
            equation = model_result["test"][problem]["task"].split("=>")[0]
            if len(model_result["test"][problem]["programs"]) > 0:
                dreamcoder_solved_problems.append((equation[len(equation.split(" ")[0])+1:], model_result["test"][problem]["programs"][0]["program"]))
    for index in range(len(test_problem_df)):
        problem_name = test_problem_df["Prefix_Eq"].iloc[index]
        f_baseline = None                                                 
        if test_problem_df["tmp"].iloc[index] in model_result_equation_nums :
            for baseline_index in range(len(baseline_model_result_df)):
                if baseline_model_result_df["Equation Number"].iloc[baseline_index] == test_problem_df["tmp"].iloc[index]:
                    f_baseline = int(baseline_model_result_df["metrics"].iloc[baseline_index])
        if f_baseline != None:
            for solved_problem, program in dreamcoder_solved_problems:
                if solved_problem == problem_name:
                    try:
                        mutual_problems+=1
                        dreamcoder_solution = replacements(program)
                        prefix_steps = evaluate(dreamcoder_solution, [problem_name], 0)[1]
                        prefix_steps.reverse()
                        prefix_steps = [problem_name]+prefix_steps
                        test_problem_df.loc[index,"dreamcoder_steps"] = str('|'.join(prefix_steps))
                        f_model = computeMetrics(prefix_steps)
                        test_problem_df.loc[index,"dreamcoder_metrics"] = str(f_model)
                        total_metric+= (f_baseline-f_model)/f_baseline
                        test_problem_df.loc[index,"dreamcoder_cscores"] = str((f_baseline-f_model)/f_baseline)
                        break
                    except Exception as e:
                        print(e)
                        print(f"Error encountered while processing problem {problem_name}")
                        print(program)
                        continue
    print(f"For {experiment_name} and a total of {mutual_problems} number of mutual problems, the average conciseness score is: {total_metric/mutual_problems}")
    return total_metric/mutual_problems

def computeLemmaCScores(test_problem_df: pd.DataFrame, lemma_result_df: pd.DataFrame, baseline_model_result_df: pd.DataFrame, experiment_name: str):
    """
    Compute the average conciseness scores of a DreamCoder experiment on the problems solved by the baseline model (ConPoLe/Lemma) in baseline_model_result_df. Ignore any problems solved by DreamCoder that are not solved by the baseline model. 

    Args:
        test_problem_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        model_result_json (str): JSON filename (frontiers.json in the last iteration of the experiment) containing results of DreamCoder.
        baseline_model_result_df (pd.DataFrame): Dataframe containing results of Conpole or Lemma.
        experiment_name (str): experiment name to be printed.

    Returns:
        float: Average conciseness score of the DreamCoder experiment on the problems solved by the baseline model.
        
    """
    total_metric = 0
    mutual_problems = 0
    test_problem_df["tmp"] = test_problem_df.index
    test_problem_df["lemma_metrics"] = ["unknown"]*len(test_problem_df)
    test_problem_df["lemma_cscores"] = ["unknown"]*len(test_problem_df)
    baseline_result_equation_nums = baseline_model_result_df["Equation Number"].to_list()
    lemma_result_equation_nums = lemma_result_df["Equation Number"].to_list()
    test_problem_df.reset_index(drop=True, inplace=True)
    for index in range(len(test_problem_df)):
        equation_num = test_problem_df["tmp"].iloc[index]
        if equation_num in lemma_result_equation_nums and equation_num in baseline_result_equation_nums:
            mutual_problems+=1
            f_model = lemma_result_df[lemma_result_df["Equation Number"]==equation_num]["metrics"].iloc[0]
            f_baseline = baseline_model_result_df[baseline_model_result_df["Equation Number"]==equation_num]["metrics"].iloc[0]
            total_metric+= (f_baseline-f_model)/f_baseline
            test_problem_df.loc[index,"dreamcoder_cscores"] = str((f_baseline-f_model)/f_baseline)
    print(f"For {experiment_name} and a total of {mutual_problems} number of mutual problems, the average conciseness score is: {total_metric/mutual_problems}")
    return total_metric/mutual_problems

def checkLemmaConpoleAccuracy(test_problem_df:pd.DataFrame, model_result_df: pd.DataFrame, model_name: str):
    """
    Check accuracy of ConPoLe/Lemma model results

    Args:
        test_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        model_result_df (pd.DataFrame): Dataframe containing results of Conpole or Lemma
        model_name (str): model name to be printed
    
    """
    count = 0
    test_problem_df["tmp"] = test_problem_df.index
    model_result_equation_nums = model_result_df["Equation Number"].to_list()
    for index in range(len(test_problem_df)):
        if test_problem_df["tmp"].iloc[index] in model_result_equation_nums:
            count+=1
    print(f"{model_name} Num Problems Solved: {count} / {len(test_problem_df)}")
    print(f"{model_name} Accuracy: {count / len(test_problem_df)}")

def checkDreamCoderAccuracy(test_problem_df:pd.DataFrame, model_result_json: str, experiment_name: str):
    """
    Check accuracy of a DreamCoder Experiment's results

    Args:
        test_df (pd.DataFrame): Dataframe containing problems on which the model's accuracy is to be calculated.
        model_result_json (str): JSON filename (frontiers.json in the last iteration of the experiment) containing results of DreamCoder.
        experiment_name (str): experiment name to be printed
    """
    count = 0
    solved_problems = []
    with open(model_result_json, "r") as f:
        model_result = json.load(f)
        for problem in model_result["train"].keys():
            if len(model_result["train"][problem]["programs"]) > 0:
                solved_problems.append(problem)
        for problem in model_result["test"].keys():
            if len(model_result["test"][problem]["programs"]) > 0:
                solved_problems.append(problem)
    for index in range(len(test_problem_df)):
        problem_name = test_problem_df["Prefix_Eq"].iloc[index]
        for solved_problem in solved_problems:
            if solved_problem.find(problem_name) != -1:
                count+=1
                break
    print(f"{experiment_name} Num Problems Solved: {count} / {len(test_problem_df)}")
    print(f"{experiment_name} Accuracy: {count / len(test_problem_df)}")

if __name__ =="__main__":
    allEqDatasets = [pd.read_csv(dataset) for dataset in ORIGINAL_DATASET_FILEPATHS]
    allEqDf = pd.concat([data for data in allEqDatasets], ignore_index=True)
    conpoleDf = pd.read_csv(GENERATED_CONPOLE_SOLNS)
    lemmaDf = pd.read_csv(GENERATED_LEMMA_SOLNS)
    numEqs = allEqDf.shape[0]
    numTrainSamples = int(numEqs*ORIGINAL_TRAIN_SPLIT)
    print("Number of training samples: ", numTrainSamples)
    print("Number of testing samples: ", numEqs - numTrainSamples)
    trainDf = allEqDf.sample(n=numTrainSamples, random_state=RANDOM_SEED)
    testDf = allEqDf.drop(trainDf.index).sample(frac=1, random_state=RANDOM_SEED)
    # checkLemmaConpoleAccuracy(testDf, conpoleDf, "ConPoLe")
    # checkLemmaConpoleAccuracy(testDf, lemmaDf, "Lemma")
    # checkLemmaConpoleAccuracy(trainDf, conpoleDf, "ConPoLe")
    # checkLemmaConpoleAccuracy(trainDf, lemmaDf, "Lemma")
    # checkDreamCoderAccuracy(testDf, REGULAR_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL")
    # checkDreamCoderAccuracy(trainDf, REGULAR_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL")
    # checkDreamCoderAccuracy(testDf, WEIGHTED_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (Weighted)")
    # checkDreamCoderAccuracy(trainDf, WEIGHTED_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (Weighted)")
    # checkDreamCoderAccuracy(testDf, GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (GD)")
    # checkDreamCoderAccuracy(trainDf, GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (GD)")
    # checkDreamCoderAccuracy(testDf, AUGMENTED_WITH_GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (AGD)")
    # checkDreamCoderAccuracy(trainDf, AUGMENTED_WITH_GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDS (AGD)")
    # goldenDatasets = [pd.read_csv(dataset) for dataset in GOLDEN_DATASET_FILEPATHS]
    # goldenDf = pd.concat([data for data in goldenDatasets], ignore_index=True)
    # checkDreamCoderAccuracy(goldenDf, GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (GD)")
    # checkDreamCoderAccuracy(goldenDf, AUGMENTED_WITH_GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (AGD)")
    # augmentedDf = pd.concat([trainDf, goldenDf], ignore_index=True)
    # checkDreamCoderAccuracy(augmentedDf, AUGMENTED_WITH_GOLDEN_DREAMCODER_STITCH_MATHDSL_JSON, "DreamCoder + Stitch + MathDSL (AGD)")
    # testDf.to_excel("meta_analysis/MathDomainAnalysis/TestDf.xlsx")
    # trainDf.to_excel("meta_analysis/MathDomainAnalysis/TrainDf.xlsx")
    # computeDreamCoderCScores(testDf, REGULAR_DREAMCODER_STITCH_MATHDSL_JSON, conpoleDf, "DreamCoder + Stitch + MathDSL (test)")
    # testDf.to_excel("meta_analysis/MathDomainAnalysis/TestRegularDreamCoderCScores.xlsx")
    # computeDreamCoderCScores(trainDf, REGULAR_DREAMCODER_STITCH_MATHDSL_JSON, conpoleDf, "DreamCoder + Stitch + MathDSL (train)")
    # trainDf.to_excel("meta_analysis/MathDomainAnalysis/TrainRegularDreamCoderCScores.xlsx")
    computeLemmaCScores(testDf, lemmaDf, conpoleDf, "Lemma (test)")
    testDf.to_excel("meta_analysis/MathDomainAnalysis/TestLemmaCScores.xlsx")
    computeLemmaCScores(trainDf, lemmaDf, conpoleDf, "Lemma (train)")
    trainDf.to_excel("meta_analysis/MathDomainAnalysis/TrainLemmaCScores.xlsx")
    # s = replacements("(lambda (#(lambda (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_simplify (mathDomain_dist $0 mathDomain_2) mathDomain_3))) (#(lambda (lambda (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_mult (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_multone (mathDomain_swap $0 $1) mathDomain_2)) mathDomain_3) mathDomain_1) mathDomain_0) mathDomain_0) mathDomain_1) mathDomain_0) mathDomain_1))) mathDomain_0 $0)))")
    # steps: list[str] = evaluate(s, ["(= (+ (1) (* (2) (x))) (+ (-3) (* (-4) (x))))"], 0)[1]
    # steps.reverse()
    # s = "(lambda (#(lambda (#(lambda (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_simplify (mathDomain_dist $0 mathDomain_2) mathDomain_3))) (mathDomain_multone (mathDomain_lrotate (#(lambda (lambda (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_simplify (mathDomain_rrotate (mathDomain_mult (#(lambda (mathDomain_simplify (mathDomain_rrotate (mathDomain_swap (mathDomain_div (mathDomain_simplify (mathDomain_rrotate (mathDomain_sub $0 mathDomain_5) mathDomain_1) mathDomain_0) mathDomain_2) mathDomain_2) mathDomain_1) mathDomain_0)) (mathDomain_multone (mathDomain_swap $0 $1) mathDomain_2)) mathDomain_3) mathDomain_1) mathDomain_0) mathDomain_0) mathDomain_1) mathDomain_0) mathDomain_1))) mathDomain_1 $0) mathDomain_1) mathDomain_6))) (mathDomain_swap $0 mathDomain_6)))"
    # eq = "(= (- (* (1) (x)) (2)) (- (* (3) (x)) (4))"
    #print(len(mdp._genSub(eq)))
    #print(computeMetrics(steps))
    # print(evaluate(replacements(s), [eq], 0))

